{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25be9d07-22e1-49a5-b45e-c5c3038ebb15",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import numpy.linalg as la\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn import linear_model\n",
    "from sklearn.kernel_approximation import RBFSampler\n",
    "\n",
    "from sklearn.linear_model import SGDRegressor\n",
    "# https://stackoverflow.com/questions/68607375/last-step-of-pipeline-should-implement-fit-or-be-the-string-passthrough\n",
    "from sklearn.pipeline import Pipeline # make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# https://stackoverflow.com/questions/53784971/how-to-disable-convergencewarning-using-sklearn\n",
    "# https://stackoverflow.com/questions/62336142/modulenotfounderror-no-module-named-sklearn-utils-testing\n",
    "from sklearn.utils._testing import ignore_warnings\n",
    "from sklearn.exceptions import ConvergenceWarning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3580ede-2739-4438-9d73-eea0904d4752",
   "metadata": {},
   "source": [
    "## (Non-)Linear function approximation\n",
    "Our goal is to test how to do function approximation. \n",
    "We consider the following functions, all with the domain $X \\equiv \\mathbb{R}^{n}$ with $n=3$:\n",
    "\n",
    "Linear:\n",
    "$$\n",
    "f_1 \\mapsto \\sum\\limits_{i=1}^{3} x_i.\n",
    "$$\n",
    "\n",
    "Quadratic\n",
    "$$\n",
    "f_2 \\mapsto \\sum\\limits_{i=1}^3 x_i^2.\n",
    "$$\n",
    "\n",
    "Simple periodic\n",
    "$$\n",
    "f_3 \\mapsto \\sin(x_1) + \\cos(x_2) + \\cos(2\\pi \\cdot x_3)\n",
    "$$\n",
    "\n",
    "Simple step-wise: (all terms must have absolute value less than 5)\n",
    "$$\n",
    "f_4 \\mapsto \\prod\\limits_{i=1}^3 \\mathbf{1}[\\vert x_i \\vert \\leq 5].\n",
    "$$\n",
    "\n",
    "Another simple step-wise:\n",
    "$$\n",
    "f_5 \\mapsto \\Big \\lfloor \\ln\\Big(1 + \\max\\big( 0, \\sum\\limits_{i=1}^j \\prod\\limits_{j \\ne i} x_j\\big) \\Big ) \\Big \\rfloor.\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d3bbd76-c86e-4e60-a7a2-88a76ac2727e",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 3\n",
    "\n",
    "def f1(x):\n",
    "    return np.sum(x)\n",
    "\n",
    "def f2(x):\n",
    "    return np.sum(np.square(x))\n",
    "\n",
    "def f3(x):\n",
    "    return np.sin(x[0]) + np.cos(x[1]) + np.cos(2*np.pi*x[2])\n",
    "\n",
    "def f4(x):\n",
    "    return np.prod((np.abs(x) <= 3).astype('int'))\n",
    "    \n",
    "def f5(x):\n",
    "    return np.floor(np.log(1+max(0, np.sum([np.prod(np.append(x[:i],x[i+1:])) for i in range(len(x))]))))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "413582a9-6cc0-4602-b95c-9b122fe98e06",
   "metadata": {},
   "source": [
    "Here, we plot on the 1D projection:\n",
    "\n",
    "$$\n",
    "\\alpha \\mapsto f_k(\\alpha \\mathbf{1}_{n}),~~ k \\in \\{1,2,3,4,5\\}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8372735-731d-441a-81b5-cddf253ad40b",
   "metadata": {},
   "outputs": [],
   "source": [
    "xs = np.linspace(-10, 10, 1000)\n",
    "ones = np.ones(n)\n",
    "ax = plt.subplot()\n",
    "ax.plot(xs, [f5(x*ones) for x in xs])\n",
    "ax.set(\n",
    "    title=\"Function values by affine transformation\", \n",
    "    xlabel=r\"$\\alpha$\", \n",
    "    ylabel=r\"$f(\\alpha \\mathbf{1}_n)$\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dd45b09-0d1c-4872-8fdc-323a660cc95b",
   "metadata": {},
   "source": [
    "## Monomial Fitting\n",
    "First we will fit by polynomials (up to degree $d$)\n",
    "\n",
    "$$\\begin{aligned}\n",
    "    v_{d\\cdot i+j}(x) &= x_i^j, ~~ i=1,\\ldots,n; j=1,\\ldots,d.\n",
    "\\end{aligned}$$\n",
    "\n",
    "Then we want to learn a weights vector $\\omega \\in \\mathbb{R}^{d \\cdot n}$ such that:\n",
    "$$\n",
    "    f_k(x) \\approx v(x)^T\\omega.\n",
    "$$\n",
    "\n",
    "We will evaluate over the domain [-10,10]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b284e3e-db80-4814-b7db-f4186d676bd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create training points and test points\n",
    "def get_polynomial_Ab(f, d, data_set):\n",
    "    A = None\n",
    "    b = np.zeros(len(data_set), dtype=float)\n",
    "    for i,data_pt in enumerate(data_set): \n",
    "        V = np.vander(data_pt, increasing=True, N=d)\n",
    "        v = np.reshape(V, newshape=(-1,))\n",
    "        if A is None:\n",
    "            A = np.zeros((len(data_set), len(v)), dtype=float)\n",
    "        A[i] = v\n",
    "        b[i] = f(data_pt)\n",
    "        \n",
    "    return A,b\n",
    "\n",
    "def get_Ab(f, n, d, n_train, n_test, get_Ab_obj, seed=None):\n",
    "    rng = np.random.default_rng(seed)\n",
    "    train_set = 20*rng.random(size=(n_train, n))-10\n",
    "    test_set  = 20*rng.random(size=(n_test, n))-10\n",
    "\n",
    "    A_train, b_train = get_Ab_obj(f, d, train_set)\n",
    "    A_test, b_test = get_Ab_obj(f, d, test_set)\n",
    "    \n",
    "    return (A_train, b_train, A_test, b_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36149555-e2a5-4e69-ad98-e4c4b20ce0ac",
   "metadata": {},
   "source": [
    "Fit the data! Our degrees are chosen by simple tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff0296e6-220d-4988-8041-b30230ad98c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "@ignore_warnings(category=ConvergenceWarning)\n",
    "def _test(n, ds, n_train, n_test, get_Ab_obj, seed, model):\n",
    "    \"\"\" Runs single trial to get accuracy of function approximation \"\"\"\n",
    "    m = len(ds)\n",
    "    fs = [f\"f{i}\" for i in range(1,m+1)]\n",
    "\n",
    "    err_train_arr = np.zeros(m, dtype=float)\n",
    "    err_test_arr = np.zeros(m, dtype=float)\n",
    "    \n",
    "    ct = 0\n",
    "    for fn,d in zip(fs, ds):\n",
    "        # TODO: Can we to locals()\n",
    "        (A_train, b_train, A_test, b_test) = get_Ab(\n",
    "            globals()[fn], \n",
    "            n, \n",
    "            d, \n",
    "            n_train, \n",
    "            n_test, \n",
    "            get_Ab_obj, \n",
    "            seed\n",
    "        )\n",
    "        model.fit(A_train,b_train)\n",
    "        # omega = model.coef_\n",
    "        omega = model.named_steps['reg'].coef_\n",
    "        # TODO: Why is behavior different than least squares?\n",
    "        # omega = la.lstsq(A_train, b_train)[0]\n",
    "\n",
    "        y_train = A_train@omega\n",
    "        y_test = A_test@omega\n",
    "        err_train = la.norm(b_train-y_train, ord=2)/n_train\n",
    "        err_test = la.norm(b_test-y_test, ord=2)/n_test\n",
    "        err_train_arr[ct] = err_train\n",
    "        err_test_arr[ct] = err_test\n",
    "        ct += 1\n",
    "        \n",
    "    return err_train_arr, err_test_arr\n",
    "\n",
    "def test(n, ds, n_train, n_test, get_Ab_obj, model, n_seeds=100):\n",
    "    \"\"\" Runs multiple trials to test the accuracy \"\"\"\n",
    "    all_err_train = np.zeros((len(ds), n_seeds), dtype=float)\n",
    "    all_err_test  = np.zeros((len(ds), n_seeds), dtype=float)\n",
    "    for i, seed in enumerate(range(n_seeds)):\n",
    "        err_train, err_test = _test(n, ds, n_train, n_test, get_Ab_obj, seed, model)\n",
    "        all_err_train[:,i] = err_train\n",
    "        all_err_test[:,i] = err_test\n",
    "\n",
    "    train_err_means = np.mean(all_err_train, axis=1)\n",
    "    train_err_stds  = np.std(all_err_train, axis=1)\n",
    "    test_err_means = np.mean(all_err_test, axis=1)\n",
    "    test_err_stds  = np.std(all_err_test, axis=1)\n",
    "\n",
    "    print(f\"{'='*12} Presenting statistics below {'='*12}\")\n",
    "    for i in range(len(ds)):\n",
    "        print(f\"f_{i+1} train error {train_err_means[i]:.2e} +/- {train_err_stds[i]:.2e}\")\n",
    "        print(f\"f_{i+1} test error {test_err_means[i]:.2e} +/- {test_err_stds[i]:.2e}\")\n",
    "        if i < len(ds)-1:\n",
    "            print(\"\")\n",
    "    print(f\"{'='*12} Finished statistics summary {'='*12}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c23d8d6d-92aa-4958-a507-785477cb5f58",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = [2,3,3,3,3]\n",
    "n_train = 40\n",
    "n_test = 20\n",
    "model = Pipeline([\n",
    "    # (\"scalar\", StandardScaler()),\n",
    "    (\"reg\", linear_model.LinearRegression())\n",
    "])\n",
    "test(n, ds, n_train, n_test, get_polynomial_Ab, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a49ccc10-9f49-47ca-8121-dfbcea41c142",
   "metadata": {},
   "source": [
    "Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a50ff96-dbe9-4532-bc75-719c4863837a",
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 3\n",
    "f = locals()[f\"f{i}\"]\n",
    "d = ds[i-1]\n",
    "seed = None\n",
    "\n",
    "# train to get weights\n",
    "rng = np.random.default_rng(seed)\n",
    "train_set = 20*rng.random(size=(n_train, n))-10\n",
    "(A_train, b_train) = get_polynomial_Ab(f, d, train_set)\n",
    "model.fit(A_train, b_train)\n",
    "omega = model.named_steps['reg'].coef_\n",
    "\n",
    "# evaluate the model\n",
    "xs = np.linspace(-10, 10, 1000)\n",
    "eval_set = np.diag(xs) @ np.ones((len(xs), n))\n",
    "(A_eval, b_eval) = get_polynomial_Ab(f, d, eval_set)\n",
    "\n",
    "# plot\n",
    "ax = plt.subplot()\n",
    "ax.plot(xs, A_eval@omega, label=r\"$\\tilde{f}$\", linestyle=\"solid\", color=\"red\")\n",
    "ax.plot(xs, b_eval, label=\"f\", linestyle=\"dashed\", color=\"black\")\n",
    "ax.set(\n",
    "    title=r\"Approximation of $f(\\alpha \\mathbf{1}_n)$\", \n",
    "    xlabel=r\"$\\alpha$\", \n",
    "    ylabel=r\"$f(\\alpha \\mathbf{1}_n)$\"\n",
    ")\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c6f761f-2a6d-4c9b-806e-a5f8fea06181",
   "metadata": {},
   "source": [
    "#### Ridge regression\n",
    "Let's try it again with regularization (i.e., ridge regression)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbbb54dc-92a7-4bc0-8a32-392baeb5ceab",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = [2,3,3,3,3]\n",
    "n_train = 100\n",
    "n_test = 20\n",
    "model = Pipeline([\n",
    "    # (\"scalar\", StandardScaler()),\n",
    "    (\"reg\", linear_model.Ridge(alpha=.1))\n",
    "])\n",
    "test(n, ds, n_train, n_test, get_polynomial_Ab, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48c93dbe-aed5-4591-8cb7-0cfa3faa5b7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 4\n",
    "f = locals()[f\"f{i}\"]\n",
    "d = ds[i-1]\n",
    "seed = None\n",
    "\n",
    "# train to get weights\n",
    "rng = np.random.default_rng(seed)\n",
    "train_set = 20*rng.random(size=(n_train, n))-10\n",
    "(A_train, b_train) = get_polynomial_Ab(f, d, train_set)\n",
    "model.fit(A_train, b_train)\n",
    "omega = model.named_steps['reg'].coef_\n",
    "\n",
    "# evaluate the model\n",
    "xs = np.linspace(-10, 10, 1000)\n",
    "eval_set = np.diag(xs) @ np.ones((len(xs), n))\n",
    "(A_eval, b_eval) = get_polynomial_Ab(f, d, eval_set)\n",
    "\n",
    "# plot\n",
    "ax = plt.subplot()\n",
    "ax.plot(xs, A_eval@omega, label=r\"$\\tilde{f}$\", linestyle=\"solid\", color=\"red\")\n",
    "ax.plot(xs, b_eval, label=\"f\", linestyle=\"dashed\", color=\"black\")\n",
    "ax.set(\n",
    "    title=r\"Approximation of $f(\\alpha \\mathbf{1}_n)$\", \n",
    "    xlabel=r\"$\\alpha$\", \n",
    "    ylabel=r\"$f(\\alpha \\mathbf{1}_n)$\"\n",
    ")\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df4280dd-7a75-422f-b357-fafa4be20332",
   "metadata": {},
   "source": [
    "## Fourier basis\n",
    "\n",
    "Recall $x \\in \\mathbb{R}^n$. Recall that we want to learn a basis $v$ such that\n",
    "$$\n",
    "f(x) \\approx v(x)^T \\omega\n",
    "$$\n",
    "for some $\\omega \\in \\mathbb{R}^d$. We define the basis\n",
    "$$\n",
    "v_i(x) = \\cos(\\pi \\cdot x^Tc^{(i)}).\n",
    "$$\n",
    "where $c^{(i)}_j \\in \\{0,\\ldots,n\\}$. Noticing there are $(n+1)^n$ possible $c^{(i)}$ since the vector can only take on discrete values, although we do not need to use all of them. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02c1f322-a646-49d5-8a7e-bfda2a57f4a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create training points and test points\n",
    "def get_fourier_Ab(f, d, data_set):\n",
    "    n = data_set.shape[1]\n",
    "    A = np.zeros((len(data_set), d), dtype=float)\n",
    "    b = np.zeros(len(data_set), dtype=float)\n",
    "    C = rng.integers(low=0, high=n+1, size=(d, n))\n",
    "    for i,data_pt in enumerate(data_set): \n",
    "        A[i] = np.cos(np.pi * C@data_pt)\n",
    "        b[i] = f(data_pt)\n",
    "\n",
    "    return (A,b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3edf2c6c-362d-4503-a38b-7b3f8b8bf3ac",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ds = 10 * np.ones(5, dtype=int)\n",
    "n_train = 100\n",
    "n_test = 20\n",
    "model = Pipeline([\n",
    "    # (\"scalar\", StandardScaler()),\n",
    "    # (\"reg\", linear_model.LinearRegression())\n",
    "    (\"reg\", linear_model.Ridge(alpha=.1))\n",
    "])\n",
    "test(n, ds, n_train, n_test, get_fourier_Ab, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d3b1726-9de0-4823-a969-5f8888d9d602",
   "metadata": {},
   "source": [
    "Here, we need to run ridge regression (i.e., regularize the weights) since the combiniatorial nature of the problem can lead to ill-conditioned problems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92d7a19b-f584-411d-8758-450eaed90b1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 3\n",
    "f = locals()[f\"f{i}\"]\n",
    "d = ds[i-1]\n",
    "seed = None\n",
    "\n",
    "# train to get weights\n",
    "rng = np.random.default_rng(seed)\n",
    "train_set = 20*rng.random(size=(n_train, n))-10\n",
    "(A_train, b_train) = get_fourier_Ab(f, d, train_set)\n",
    "model.fit(A_train, b_train)\n",
    "omega = model.named_steps['reg'].coef_\n",
    "\n",
    "# evaluate the model\n",
    "xs = np.linspace(-10, 10, 1000)\n",
    "eval_set = np.diag(xs) @ np.ones((len(xs), n))\n",
    "(A_eval, b_eval) = get_fourier_Ab(f, d, eval_set)\n",
    "\n",
    "# plot\n",
    "ax = plt.subplot()\n",
    "ax.plot(xs, A_eval@omega, label=r\"$\\tilde{f}$\", linestyle=\"solid\", color=\"red\")\n",
    "ax.plot(xs, b_eval, label=\"f\", linestyle=\"dashed\", color=\"black\")\n",
    "ax.set(\n",
    "    title=r\"Approximation of $f(\\alpha \\mathbf{1}_n)$\", \n",
    "    xlabel=r\"$\\alpha$\", \n",
    "    ylabel=r\"$f(\\alpha \\mathbf{1}_n)$\"\n",
    ")\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1735476f-971e-4b0a-93b5-0c3f28c83e96",
   "metadata": {},
   "source": [
    "### Radius basis function approximation by random Fourier features (i.e., random kitchen sinks)\n",
    "Similar to Fourier basis,  we define the basis\n",
    "$$\n",
    "v_i(x) = \\sqrt{\\frac{2}{d}} \\cos(x^Tc^{(i)} + b^{(i)})\n",
    "$$\n",
    "where $c^{(i)} \\in \\mathbb{R}^n$ and $b^{(i)} \\in \\mathbb{R}$, i.e., continuous (c.f. Fourier basis required $b^{(i)} = 0$ and $c^{(i)}$ to be integers).\n",
    "\n",
    "The way the vectors/scalars are selected are by, for some $\\gamma$ (default $\\gamma=1$ in [RBFSampler](https://scikit-learn.org/stable/modules/generated/sklearn.kernel_approximation.RBFSampler.html#sklearn.kernel_approximation.RBFSampler))\n",
    "$$\\begin{aligned}\n",
    "c^{(i)}_j &= \\sqrt{2\\gamma} \\cdot z_{i,j}, ~~~ z_{i,j} \\sim \\mathcal{N}(0,1) \\\\\n",
    "b^{(i)} &\\sim \\mathrm{Uni}[0, 2\\pi].\n",
    "\\end{aligned}$$\n",
    "\n",
    "In the linked code above, the default dimension is $d=100$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f0c2d40-17de-4409-914e-e7e599587333",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_rbf_Ab(f, n, d, n_train, n_test, seed=None):\n",
    "#     \"\"\" Radial basis function \"\"\"\n",
    "#     rng = np.random.default_rng(seed)\n",
    "#     train_set = 20*rng.random(size=(n_train, n))-10\n",
    "#     test_set  = 20*rng.random(size=(n_test, n))-10\n",
    "#     \n",
    "#     A_train = np.zeros((n_train, d), dtype=float)\n",
    "#     A_test  = np.zeros((n_test, d), dtype=float)\n",
    "#     b_train = np.zeros(n_train, dtype=float)\n",
    "#     b_test  = np.zeros(n_test, dtype=float)\n",
    "#     \n",
    "#     C = 2**0.5 * rng.normal(size=(d,n))\n",
    "#     b = 2*np.pi*rng.random(size=d)\n",
    "#     \n",
    "#     for i,train_pt in enumerate(train_set): \n",
    "#         A_train[i] = (2./d)**0.5 * np.cos(C@train_pt+b)\n",
    "#         b_train[i] = f(train_pt)\n",
    "# \n",
    "#     for i,test_pt in enumerate(test_set):\n",
    "#         A_test[i] = (2./d)**0.5 * np.cos(C@test_pt+b)\n",
    "#         b_test[i] = f(test_pt)\n",
    "# \n",
    "#     return (A_train, b_train, A_test, b_test)\n",
    "\n",
    "# create training points and test points\n",
    "\n",
    "def get_rbf_Ab(f, d, data_set):\n",
    "    n = data_set.shape[1]\n",
    "    A = np.zeros((len(data_set), d), dtype=float)\n",
    "    b = np.zeros(len(data_set), dtype=float)\n",
    "    C = 2**0.5 * rng.normal(size=(d,n))\n",
    "    s = 2*np.pi*rng.random(size=d)\n",
    "    for i,data_pt in enumerate(data_set): \n",
    "        A[i] = (2./d)**0.5 * np.cos(C@data_pt+s)\n",
    "        b[i] = f(data_pt)\n",
    "\n",
    "    return (A,b)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cbbdde9-04bd-4956-871c-a0d42894d3cd",
   "metadata": {},
   "source": [
    "Below we consider different dimensions (`ds` is an array of dimension for each function) and training data (`n_train`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0fce880-09ab-4b5a-991e-c1a2944f6320",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = 100 * np.ones(5, dtype=int)\n",
    "n_train = 40\n",
    "n_test = 20\n",
    "model = Pipeline([\n",
    "    # (\"scalar\", StandardScaler()),\n",
    "    (\"reg\", linear_model.LinearRegression())\n",
    "    # (\"reg\", linear_model.Ridge(alpha=.1))\n",
    "])\n",
    "test(n, ds, n_train, n_test, get_rbf_Ab, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cc9d5d3-87a2-4b69-b0f5-c9ef24a3f9d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 3\n",
    "f = locals()[f\"f{i}\"]\n",
    "d = ds[i-1]\n",
    "seed = None\n",
    "\n",
    "# train to get weights\n",
    "rng = np.random.default_rng(seed)\n",
    "train_set = 20*rng.random(size=(n_train, n))-10\n",
    "(A_train, b_train) = get_rbf_Ab(f, d, train_set)\n",
    "model.fit(A_train, b_train)\n",
    "omega = model.named_steps['reg'].coef_\n",
    "\n",
    "# evaluate the model\n",
    "xs = np.linspace(-10, 10, 1000)\n",
    "eval_set = np.diag(xs) @ np.ones((len(xs), n))\n",
    "(A_eval, b_eval) = get_fourier_Ab(f, d, eval_set)\n",
    "\n",
    "# plot\n",
    "ax = plt.subplot()\n",
    "ax.plot(xs, A_eval@omega, label=r\"$\\tilde{f}$\", linestyle=\"solid\", color=\"red\")\n",
    "ax.plot(xs, b_eval, label=\"f\", linestyle=\"dashed\", color=\"black\")\n",
    "ax.set(\n",
    "    title=r\"Approximation of $f(\\alpha \\mathbf{1}_n)$\", \n",
    "    xlabel=r\"$\\alpha$\", \n",
    "    ylabel=r\"$f(\\alpha \\mathbf{1}_n)$\"\n",
    ")\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7ab1b53-2d9e-44da-8f3c-5d73391f8627",
   "metadata": {},
   "source": [
    "#### Compare to sklearn\n",
    "\n",
    "We will now use sklearn's implementation of RBF. We hack the code to give us features for both training and testing data by appending both data when computing the feature matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e277d8cc-bfef-4937-b98c-786130c8c5ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_sklearn_rbf_Ab(f, n, d, n_train, n_test, seed=None):\n",
    "#     \"\"\" Radial basis function \"\"\"\n",
    "#     rng = np.random.default_rng(seed)\n",
    "#     train_set = 20*rng.random(size=(n_train, n))-10\n",
    "#     test_set  = 20*rng.random(size=(n_test, n))-10\n",
    "#     \n",
    "#     rbf_feature = RBFSampler(n_components=d, gamma=1, random_state=1)\n",
    "#     \n",
    "#     # hack to get features for both training and testing data\n",
    "#     data_set = np.vstack((train_set, test_set))\n",
    "#     C = rbf_feature.fit_transform(data_set)\n",
    "#     A_train = C[:len(train_set)]\n",
    "#     A_test = C[len(train_set):]\n",
    "#     b_train = np.zeros(n_train, dtype=float)\n",
    "#     b_test  = np.zeros(n_test, dtype=float)\n",
    "#     \n",
    "#     for i,train_pt in enumerate(train_set): \n",
    "#         b_train[i] = f(train_pt)\n",
    "# \n",
    "#     for i,test_pt in enumerate(test_set):\n",
    "#         b_test[i] = f(test_pt)\n",
    "# \n",
    "#     return (A_train, b_train, A_test, b_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f0fdc1e-a654-4deb-9741-ff4c0a4f63dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ds = 100 * np.ones(5, dtype=int)\n",
    "# n_train = 40\n",
    "# n_test = 20\n",
    "# model = Pipeline([\n",
    "#     # (\"scalar\", StandardScaler()),\n",
    "#     (\"reg\", linear_model.LinearRegression())\n",
    "# ])\n",
    "# test(n, ds, n_train, n_test, get_sklearn_rbf_Ab, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b45a931b-a013-4db6-a8ef-b5af22cedc8d",
   "metadata": {},
   "source": [
    "Comparing the two datasets (i.e., ours vs sklearn), we have approximately similar training and testing errors.\n",
    "\n",
    "**EDIT**: Removed since we changed how to generate (A,b)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e801a5b4-06c5-4b09-aadc-3229bd50c7fd",
   "metadata": {},
   "source": [
    "### Training with SGD\n",
    "In both the monomial and random kitchen sink, we used a least squares estimator (via `LinearRegression`) to fit the data. We can also consider stochastic gradient descent, or SGD for short. Let us consider that now, starting again with monomial basis and RBF with random Fourier features. We will use [SGDRegressor](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.SGDRegressor.html#sklearn.linear_model.SGDRegressor)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f100c7db-fc22-4d0c-b277-e4d5d4d1e1c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = 100 * np.ones(5, dtype=int)\n",
    "n_train = 40\n",
    "n_test = 20\n",
    "model = Pipeline([\n",
    "    (\"scalar\", StandardScaler()),\n",
    "    (\"reg\", SGDRegressor(max_iter=1000, tol=1e-3, learning_rate=\"optimal\", eta0=1e-1))\n",
    "])\n",
    "test(n, ds, n_train, n_test, get_polynomial_Ab, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b6d3e3e-5268-4849-86f3-b5679f1d0a83",
   "metadata": {},
   "source": [
    "**It seems polynomial does not play with with SGD. Likely due to the high condition number.**\n",
    "\n",
    "Let us now consider RBF. We consider scaling and no scaling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "050967a5-7b34-48ec-a99e-5d86d3739738",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = 100 * np.ones(5, dtype=int)\n",
    "n_train = 40\n",
    "n_test = 20\n",
    "model = Pipeline([\n",
    "    (\"reg\", SGDRegressor(max_iter=1000, tol=1e-3))\n",
    "])\n",
    "test(n, ds, n_train, n_test, get_rbf_Ab, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99417218-c57f-46e9-9969-ee6b444568a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = 100 * np.ones(5, dtype=int)\n",
    "n_train = 40\n",
    "n_test = 20\n",
    "model = Pipeline([\n",
    "    (\"scalar\", StandardScaler()),\n",
    "    (\"reg\", SGDRegressor(max_iter=1000, tol=1e-3))\n",
    "])\n",
    "# model = SGDRegressor(max_iter=1000, tol=1e-3)\n",
    "test(n, ds, n_train, n_test, get_rbf_Ab, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0137dae1-d29a-4735-8a67-1446b3c4770e",
   "metadata": {},
   "source": [
    "Let's play around with the learning rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97ef7176-489b-437e-b591-89c97a8dbe32",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = 100 * np.ones(5, dtype=int)\n",
    "n_train = 40\n",
    "n_test = 20\n",
    "model = Pipeline([\n",
    "    (\"scalar\", StandardScaler()),\n",
    "    (\"reg\", SGDRegressor(\n",
    "        max_iter=1000, \n",
    "        tol=1e-3, \n",
    "        learning_rate=\"constant\", \n",
    "        eta0=1e-2\n",
    "    ))\n",
    "])\n",
    "test(n, ds, n_train, n_test, get_rbf_Ab, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83d10e34-e4c1-4067-9666-ac4016803fea",
   "metadata": {},
   "source": [
    "## Tiling Coding (Course Coding)\n",
    "\n",
    "Tiling coding is a type of course coding, where binary features are activitated (either 0 or 1) depending on whether the point is within a certain tile.\n",
    "\n",
    "Mathematically, for every $i$, there exists a unique $x_i \\in X$ such that\n",
    "$$\\begin{aligned} v_i(x) = \\begin{cases} 1 & : x \\in \\mathrm{tile}(x_i) \\\\ 0 & : \\text{o.w.} \\end{cases} \\end{aligned}.$$\n",
    "$$\n",
    "Here, each $v_i$ decomposes the domain $X$ into (equal length) tiles, or hypercubes. We assume the decomposition of $X$ for $v_i$ and $v_j$ shifted by a constant that is a fraction of the tile width along all dimension (to prevent overlap in any dimension).\n",
    "\n",
    "Such a feature mapping can be useful where there are geographically based features. For example, if to capture a function that gets larger as one gets closer to an origin, we define features that take on values of 1 only close to the center.\n",
    "\n",
    "**I will not code this since the next feature is a smoothed/continuous version of tiling coding**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "736fc5f4-7667-4de6-b2db-df930bbbc31d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "710a171f-e054-4b4a-814d-3944ae7330aa",
   "metadata": {},
   "source": [
    "## Brief Tour: What is Reproducing Kernel Hilbert Space?\n",
    "\n",
    "Recall that we want to approximate a function $f : X \\mapsto \\mathbb{R}$ via\n",
    "$$\n",
    "f(x) \\approx v(x)^T\\omega\n",
    "$$\n",
    "for some $\\omega \\in \\mathbb{R}^d$. For example, in linear SVM, we let $v = I$ and define the classifier as\n",
    "$$\n",
    "\\mathrm{sign}(\\omega^Tx - b).\n",
    "$$\n",
    "However, many data sets may not be linearly separable (i.e., separated by a hyperplane), so we need a nonlinear seperation. We want to lift $x \\mapsto \\varphi(x)$, where $\\varphi : X \\mapsto H \\subseteq \\mathbb{R}^d$ for some $d > n$, where $H$ is a Hilbert space. Then we can compute instead\n",
    "$$\n",
    "\\omega^Tx \\to \\langle \\omega, \\varphi(x) \\rangle_H.\n",
    "$$\n",
    "Here, we abused notation and assume $\\omega$ is defined according the respective space.\n",
    "Often times, the larger the lifted space $d$ is (possibly infinite dimensional), the better approximation power we get.\n",
    "So while this lifting procedure is appealing, it can be computationally intractble when $d$ is large.\n",
    "\n",
    "The idea of Reproducing Kernel Hilbert Space (RKHS) is to use a kernel $k: H \\times H \\to \\mathbb{R}$ and the *kernel trick* to avoid the need to explicitly compute $\\varphi$. The kernel trick is a consequence of *Mercer's Theorem*, which says under suitable conditions for a lifting $\\varphi$, there exists a kernel $k$ such that \n",
    "$$\n",
    "k(x,x') = \\langle \\varphi(x), \\varphi(x') \\rangle_H, ~~ \\forall x,x' \\in X.\n",
    "$$\n",
    "This is a sufficient result, so it holds both ways. The key insight here is if we can find $k$, we do not need to form $\\varphi$, which can be very high-dimensional.\n",
    "\n",
    "Let us now apply this kernel trick to function approximation. Suppose we already have some data $\\{(x_i,y_i=f(x_i))\\}$, and we want to approximate $f$. Let us we write our weights in Hilbert space (i.e., $y_i \\cdot \\varphi(x_i)$ is the basis -- **WHY??**)\n",
    "$$\n",
    "\\omega = \\sum\\limits_{i} \\alpha_i y_i \\varphi(x_i).\n",
    "$$\n",
    "Using the kernel trick,\n",
    "$$\n",
    "\\langle \\omega, \\varphi(x) \\rangle_H  = \\sum\\limits_i \\alpha_i y_i \\underbrace{\\langle \\varphi(x_i), \\varphi(x) \\rangle_H}_{k(x_i,x)}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b53062a2-a137-43ee-a9c7-83ea48164158",
   "metadata": {},
   "source": [
    "## Radial Basis Function\n",
    "\n",
    "We define the basis\n",
    "$$\n",
    "v_i(x) = k(x,c^{(i)}) := \\mathrm{exp}\\Big( \\frac{\\|x-c^{(i)}\\|^2}{2\\sigma_i^2} \\Big).\n",
    "$$\n",
    "\n",
    "Notice that $v_i(x) \\in (0,1]$, hence this is a relaxed version of a binary feature. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "595308bd-92a9-437f-a36e-89013f196aee",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
