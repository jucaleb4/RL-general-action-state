{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25be9d07-22e1-49a5-b45e-c5c3038ebb15",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import numpy.linalg as la\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn import linear_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3580ede-2739-4438-9d73-eea0904d4752",
   "metadata": {},
   "source": [
    "## (Non-)Linear function approximation\n",
    "Our goal is to test how to do function approximation. \n",
    "We consider the following functions, all with the domain $X \\equiv \\mathbb{R}^{n}$ with $n=3$:\n",
    "\n",
    "Linear:\n",
    "$$\n",
    "f_1 \\mapsto \\sum\\limits_{i=1}^{3} x_i.\n",
    "$$\n",
    "\n",
    "Quadratic\n",
    "$$\n",
    "f_2 \\mapsto \\sum\\limits_{i=1}^3 x_i^2.\n",
    "$$\n",
    "\n",
    "Simple periodic\n",
    "$$\n",
    "f_3 \\mapsto \\sin(x_1) + \\cos(x_2) + \\sin(x_3)\\cos(x_3)\n",
    "$$\n",
    "\n",
    "Simple step-wise: (all terms must have absolute value less than 5)\n",
    "$$\n",
    "f_4 \\mapsto \\prod\\limits_{i=1}^3 \\mathbf{1}[\\vert x_i \\vert \\leq 5].\n",
    "$$\n",
    "\n",
    "Another simple step-wise:\n",
    "$$\n",
    "f_5 \\mapsto \\Big \\lfloor \\ln\\Big(1 + \\max\\big( 0, \\sum\\limits_{i=1}^j \\prod\\limits_{j \\ne i} x_j\\big) \\Big ) \\Big \\rfloor.\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d3bbd76-c86e-4e60-a7a2-88a76ac2727e",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 3\n",
    "\n",
    "def f1(x):\n",
    "    return np.sum(x)\n",
    "\n",
    "def f2(x):\n",
    "    return np.sum(np.square(x))\n",
    "\n",
    "def f3(x):\n",
    "    return np.sin(x[0]) + np.cos(x[1]) + np.sin(x[2]) * np.cos(x[2])\n",
    "\n",
    "def f4(x):\n",
    "    return np.prod((np.abs(x) <= 5).astype('int'))\n",
    "    \n",
    "def f5(x):\n",
    "    return np.floor(np.log(1+max(0, np.sum([np.prod(np.append(x[:i],x[i+1:])) for i in range(len(x))]))))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "413582a9-6cc0-4602-b95c-9b122fe98e06",
   "metadata": {},
   "source": [
    "Here, we plot on the 1D projection:\n",
    "\n",
    "$$\n",
    "\\alpha \\mapsto f_k(\\alpha \\mathbf{1}_{n}),~~ k \\in \\{1,2,3,4,5\\}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8372735-731d-441a-81b5-cddf253ad40b",
   "metadata": {},
   "outputs": [],
   "source": [
    "xs = np.linspace(-10, 10, 1000)\n",
    "ones = np.ones(n)\n",
    "ax = plt.subplot()\n",
    "ax.plot(xs, [f5(x*ones) for x in xs])\n",
    "ax.set(\n",
    "    title=\"Function values by affine transformation\", \n",
    "    xlabel=r\"$\\alpha$\", \n",
    "    ylabel=r\"$f(\\alpha \\mathbf{1}_n)$\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dd45b09-0d1c-4872-8fdc-323a660cc95b",
   "metadata": {},
   "source": [
    "## Monomial Fitting\n",
    "First we will fit by polynomials (up to degree $d$)\n",
    "\n",
    "$$\\begin{aligned}\n",
    "    v_{d\\cdot i+j}(x) &= x_i^j, ~~ i=1,\\ldots,n; j=1,\\ldots,d.\n",
    "\\end{aligned}$$\n",
    "\n",
    "Then we want to learn a weights vector $\\omega \\in \\mathbb{R}^{d \\cdot n}$ such that:\n",
    "$$\n",
    "    f_k(x) \\approx v(x)^T\\omega.\n",
    "$$\n",
    "\n",
    "We will evaluate over the domain [-50,50]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b284e3e-db80-4814-b7db-f4186d676bd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create training points and test points\n",
    "def get_Ab(f, d, seed=None):\n",
    "    n_train = 4*d*n\n",
    "    n_test  = d*n\n",
    "    \n",
    "    rng = np.random.default_rng(seed)\n",
    "    train_set = 10*rng.random(size=(n_train, n))-5\n",
    "    test_set  = 10*rng.random(size=(n_test, n))-5\n",
    "    \n",
    "    A_train = np.zeros((n_train, d*n), dtype=float)\n",
    "    A_test  = np.zeros((n_test, d*n), dtype=float)\n",
    "    b_train = np.zeros(n_train, dtype=float)\n",
    "    b_test  = np.zeros(n_test, dtype=float)\n",
    "    \n",
    "    for i,train_pt in enumerate(train_set): \n",
    "        V = np.vander(train_pt, increasing=True, N=d)\n",
    "        A_train[i] = np.reshape(V, newshape=(-1,))\n",
    "        b_train[i] = f(train_pt)\n",
    "\n",
    "    for i,test_pt in enumerate(test_set):\n",
    "        V = np.vander(test_pt, increasing=True, N=d)\n",
    "        A_test[i] = np.reshape(V, newshape=(-1,))\n",
    "        b_test[i] = f(test_pt)\n",
    "\n",
    "    return (A_train, b_train, A_test, b_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36149555-e2a5-4e69-ad98-e4c4b20ce0ac",
   "metadata": {},
   "source": [
    "Fit the data! Our degrees are chosen by simple tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff0296e6-220d-4988-8041-b30230ad98c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "fs = [f\"f{i}\" for i in range(1,6)]\n",
    "ds = [2,3,3,3,3]\n",
    "\n",
    "for fn,d in zip(fs, ds):\n",
    "    (A_train, y_train, A_test, y_test) = get_Ab(locals()[fn], d)\n",
    "    reg = linear_model.LinearRegression()\n",
    "    reg.fit(A_train,y_train)\n",
    "    omega = reg.coef_\n",
    "    \n",
    "    y = A_test@omega\n",
    "    print(f\"Error for {fn} with degree d={d}: {la.norm(y-y_test, ord=2)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c6f761f-2a6d-4c9b-806e-a5f8fea06181",
   "metadata": {},
   "source": [
    "Let's try it again with regularization (i.e., ridge regression)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbbb54dc-92a7-4bc0-8a32-392baeb5ceab",
   "metadata": {},
   "outputs": [],
   "source": [
    "fs = [f\"f{i}\" for i in range(1,6)]\n",
    "ds = [2,3,3,3,3]\n",
    "\n",
    "for fn,d in zip(fs, ds):\n",
    "    (A_train, y_train, A_test, y_test) = get_Ab(locals()[fn], d)\n",
    "    reg = linear_model.Ridge(alpha=.1)\n",
    "    reg.fit(A_train,y_train)\n",
    "    omega = reg.coef_\n",
    "    \n",
    "    y = A_test@omega\n",
    "    print(f\"Error for {fn} with degree d={d}: {la.norm(y-y_test, ord=2)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1735476f-971e-4b0a-93b5-0c3f28c83e96",
   "metadata": {},
   "source": [
    "## Fourier basis\n",
    "\n",
    "Recall $x \\in \\mathbb{R}^n$. We define the basis\n",
    "$$\n",
    "v_i(x) = \\cos(\\pi \\cdot x^Tc^{(i)}).\n",
    "$$\n",
    "where $c^{(i)}_j \\in \\{0,\\ldots,n\\}$. Noticing there are $(n+1)^n$ possible $c^{(i)}$ since the vector can only take on discrete values, although we do not need to use all of them. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f0c2d40-17de-4409-914e-e7e599587333",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "83d10e34-e4c1-4067-9666-ac4016803fea",
   "metadata": {},
   "source": [
    "## Tiling Coding (Course Coding)\n",
    "\n",
    "Tiling coding is a type of course coding, where binary features are activitated (either 0 or 1) depending on whether the point is within a certain tile.\n",
    "\n",
    "Mathematically, for every $i$, there exists a unique $x_i \\in X$ such that\n",
    "$$\\begin{aligned} v_i(x) = \\begin{cases} 1 & : x \\in \\mathrm{tile}(x_i) \\\\ 0 & : \\text{o.w.} \\end{cases} \\end{aligned}.$$\n",
    "$$\n",
    "Here, each $v_i$ decomposes the domain $X$ into (equal length) tiles, or hypercubes. We assume the decomposition of $X$ for $v_i$ and $v_j$ shifted by a constant that is a fraction of the tile width along all dimension (to prevent overlap in any dimension).\n",
    "\n",
    "Such a feature mapping can be useful where there are geographically based features. For example, if to capture a function that gets larger as one gets closer to an origin, we define features that take on values of 1 only close to the center.\n",
    "\n",
    "**I will not code this since the next feature is a smoothed/continuous version of tiling coding**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "736fc5f4-7667-4de6-b2db-df930bbbc31d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "710a171f-e054-4b4a-814d-3944ae7330aa",
   "metadata": {},
   "source": [
    "## Brief Tour: What is Reproducing Kernel Hilbert Space?\n",
    "\n",
    "Recall that we want to approximate a function $f : X \\mapsto \\mathbb{R}$ via\n",
    "$$\n",
    "f(x) \\approx v(x)^T\\omega\n",
    "$$\n",
    "for some $\\omega \\in \\mathbb{R}^d$. For example, in linear SVM, we let $v = I$ and define the classifier as\n",
    "$$\n",
    "\\mathrm{sign}(\\omega^Tx - b).\n",
    "$$\n",
    "However, many data sets may not be linearly separable (i.e., separated by a hyperplane), so we need a nonlinear seperation. We want to lift $x \\mapsto \\varphi(x)$, where $\\varphi : X \\mapsto H \\subseteq \\mathbb{R}^d$ for some $d > n$, where $H$ is a Hilbert space. Then we can compute instead\n",
    "$$\n",
    "\\omega^Tx \\to \\langle \\omega, \\varphi(x) \\rangle_H.\n",
    "$$\n",
    "Here, we abused notation and assume $\\omega$ is defined according the respective space.\n",
    "Often times, the larger the lifted space $d$ is (possibly infinite dimensional), the better approximation power we get.\n",
    "So while this lifting procedure is appealing, it can be computationally intractble when $d$ is large.\n",
    "\n",
    "The idea of Reproducing Kernel Hilbert Space (RKHS) is to use a kernel $k: H \\times H \\to \\mathbb{R}$ and the *kernel trick* to avoid the need to explicitly compute $\\varphi$. The kernel trick is a consequence of *Mercer's Theorem*, which says under suitable conditions for a lifting $\\varphi$, there exists a kernel $k$ such that \n",
    "$$\n",
    "k(x,x') = \\langle \\varphi(x), \\varphi(x') \\rangle_H, ~~ \\forall x,x' \\in X.\n",
    "$$\n",
    "This is a sufficient result, so it holds both ways. The key insight here is if we can find $k$, we do not need to form $\\varphi$, which can be very high-dimensional.\n",
    "\n",
    "Let us now apply this kernel trick to function approximation. Suppose we already have some data $\\{(x_i,y_i=f(x_i))\\}$, and we want to approximate $f$. Let us we write our weights in Hilbert space (i.e., $y_i \\cdot \\varphi(x_i)$ is the basis -- **WHY??**)\n",
    "$$\n",
    "\\omega = \\sum\\limits_{i} \\alpha_i y_i \\varphi(x_i).\n",
    "$$\n",
    "Using the kernel trick,\n",
    "$$\n",
    "\\langle \\omega, \\varphi(x) \\rangle_H  = \\sum\\limits_i \\alpha_i y_i \\underbrace{\\langle \\varphi(x_i), \\varphi(x) \\rangle_H}_{k(x_i,x)}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b53062a2-a137-43ee-a9c7-83ea48164158",
   "metadata": {},
   "source": [
    "## Radial Basis Function\n",
    "\n",
    "We define the basis\n",
    "$$\n",
    "v_i(x) = k(x,c^{(i)}) := \\mathrm{exp}\\Big( \\frac{\\|x-c^{(i)}\\|^2}{2\\sigma_i^2} \\Big).\n",
    "$$\n",
    "\n",
    "Notice that $v_i(x) \\in (0,1]$, hence this is a relaxed version of a binary feature. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "595308bd-92a9-437f-a36e-89013f196aee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "12bff880-8ad8-4c6b-8cf6-c8f60805d6ef",
   "metadata": {},
   "source": [
    "## Random Kitchen Sinks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "855e0269-716e-49c3-8805-4cb190313320",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
